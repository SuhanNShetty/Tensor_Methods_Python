{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: \n",
    "    1. \"Tensor Decompositions and Applications\", Tamara Kolda, Brett W. Bader, \n",
    "    2. Multi-way Analysis by Age Smilde, R Bros \n",
    "Author: Suhan Shetty <suhan.n.shetty@idiap.ch>\n",
    "\n",
    "Date: 29th July 2019\n",
    "\n",
    "This is an implementation of common tools for 3-way array\n",
    "    - Generate a 3-way tensor data {Input(X),Output(Y)}\n",
    "    - Tensor Operations\n",
    "    - Tensor Decomposition Techniques\n",
    "    - Tensor-Tensor Regression\n",
    "    - Array-normal distribution\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import multi_dot as mat_mul\n",
    "from numpy.random import random_sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"   \n",
    "A note about convention used:\n",
    "numpy array index starts from 0. However, we use for the mode numbering startion from 1.\n",
    "So, axis 0 in a numpy array corresponds to mode 1.\n",
    "\"\"\"\n",
    "\n",
    "class array3:\n",
    "    \n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "    # Genrate a random 3-way array\n",
    "    def random_array3(self,tensor_shape):\n",
    "        #tensor_shape.reverse()\n",
    "        t_shape = [tensor_shape[2],tensor_shape[0],tensor_shape[1]]\n",
    "        X = 0\n",
    "        R = np.random.randint(10)+10 # Randomize R, number of true components\n",
    "        for p in range(R): # Add some true components to the tensor\n",
    "            a = random_sample((tensor_shape[0],))\n",
    "            b = random_sample((tensor_shape[1],))\n",
    "            c = random_sample((tensor_shape[2],))\n",
    "            X = X + self.outer_product(a,b,c) \n",
    "        X = X + np.random.random(t_shape) # Add some noise over the true component\n",
    "        return X\n",
    "\n",
    "\n",
    "# Shape of the tensor --------------------------------------------------------------------------------------------\n",
    "\n",
    "    def shape(self,X): #\n",
    "        return (X.shape[1],X.shape[2],X.shape[0])\n",
    "\n",
    "# Compute kronecker product of the set of matrices ---------------------------------------------------------------\n",
    "    \n",
    "    def kron(self,A): # Here, A=[A1,A2,A3,..,AN]\n",
    "        N = len(A)\n",
    "        kp = np.kron(A[0],A[1])\n",
    "        k = 2\n",
    "        while k<N:\n",
    "            kp = np.kron(kp,A[k])\n",
    "            k=k+1\n",
    "        return kp\n",
    "    \n",
    "# Unfold the tensor along the given mode --------------------------------------------------------------------------\n",
    "   \n",
    "    def unfold(self,X,n):\n",
    "        if n==2:\n",
    "            X = np.swapaxes(X,1,2) \n",
    "        elif n==3:\n",
    "            X = np.swapaxes(X,0,2)\n",
    "            X = np.swapaxes(X,1,2)\n",
    "        depth = X.shape[0]\n",
    "        Xn = X[0,:,:] \n",
    "        for k in range(depth-1):\n",
    "            tmp = X[k+1,:,:]\n",
    "            Xn = np.concatenate((Xn,tmp),axis=1)\n",
    "        return Xn\n",
    "    \n",
    "# Unfold the tensor along the given mode for 4-way array ---------------------------------------------------------\n",
    "   \n",
    "    def unfold4(self,X,n):\n",
    "        if n==1:\n",
    "            m = X.shape[2]\n",
    "            X = np.swapaxes(X,0,2)\n",
    "            X = np.swapaxes(X,1,2)\n",
    "        elif n==2:\n",
    "            m = X.shape[3]\n",
    "            X = np.swapaxes(X,0,3)\n",
    "            X = np.swapaxes(X,1,3)\n",
    "            X = np.swapaxes(X,2,3)\n",
    "        elif n==3:\n",
    "            \n",
    "            m = X.shape[1]\n",
    "            X = np.swapaxes(X,0,1)\n",
    "            \n",
    "        elif n==4:\n",
    "            m = X.shape[0]\n",
    "\n",
    "       \n",
    "        Xn = X.flatten('C')\n",
    "        l = len(Xn)\n",
    "        Xn = np.reshape(Xn,[m,int(l/m)])\n",
    "        return Xn\n",
    "    \n",
    "# mode-n product of the tensor with a matrix-----------------------------------------------------------------------\n",
    "   \n",
    "    # For 3-way tensor\n",
    "    def mode_n_product(self,X,M,n): # T_out = T_in * M\n",
    "        if n==2:\n",
    "            X = np.einsum('ijl,kl',X,M)\n",
    "        elif n==1:\n",
    "            X = np.einsum('ilk,jl',X,M)\n",
    "        elif n==3:\n",
    "            X = np.einsum('ljk,il',X,M)\n",
    "        return X\n",
    "    # For 4-way tensor\n",
    "    def mode_n_product4(self,X,M,n): # T_out = T_in * M\n",
    "        if n==2:\n",
    "            X = np.einsum('hijl,kl',X,M)\n",
    "        elif n==1:\n",
    "            X = np.einsum('hilk,jl',X,M)\n",
    "        elif n==3:\n",
    "            X = np.einsum('hljk,il',X,M)\n",
    "        elif n==4:\n",
    "            X = np.einsum('lijk,hl',X,M)\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "# Compute the Frobenius norm -------------------------------------------------------------------------------------\n",
    "   \n",
    "    def norm(self,X):\n",
    "        return np.linalg.norm(X.flatten())\n",
    "\n",
    "# Vectorize a tensor----------------------------------------------------------------------------------------------\n",
    "   \n",
    "    def vectorize(self,X):\n",
    "        return X.flatten()\n",
    "            \n",
    "# Khatri-Rao product of two matrices------------------------------------------------------------------------------\n",
    "    \n",
    "    def khatri_rao(self,M):\n",
    "        X = M[0]\n",
    "        Y = M[1]\n",
    "        assert X.shape[1]==Y.shape[1],\"Number of columns must match\"\n",
    "        I = X.shape[0]*Y.shape[0]\n",
    "        J = X.shape[1]\n",
    "        Z = np.empty([I,J])      \n",
    "        for k in range(J):\n",
    "            Z[:,k] = np.kron(X[:,k],Y[:,k])\n",
    "        return Z\n",
    "\n",
    "# Hadamard Product -----------------------------------------------------------------------------------------------\n",
    "\n",
    "    def hadamard(self,X,Y):\n",
    "        assert X.shape==Y.shape,\"Dimensions must match\"\n",
    "        return X*Y\n",
    "\n",
    "# Outer Product ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def outer_product(self,a,b,c): #outer product of three vectors\n",
    "        assert a.ndim == 1 and b.ndim==1 and c.ndim==1, \"Input has to be 1d arrays\"\n",
    "        return np.einsum('i,k,j',c,b,a) # or np.einsum('i,j,k',a,b,c).swapaxes(0,2)\n",
    "\n",
    "\n",
    "# CP/CANDECOMP/PARAFAC decompostion of a tensor ------------------------------------------------------------------\n",
    "    # Uses ALS technique to decompose the tensor into R rank-1 tensors\n",
    "    def cp(self,X,R):\n",
    "        I = self.shape(X)\n",
    "        r = min([I[0]*I[1], I[1]*I[2], I[0]*I[2]])\n",
    "        assert R<r, \"Reduce R. Its too large comapred to the dimension of the data\"\n",
    "             \n",
    "        # Initialization of factor matrices\n",
    "        A = [np.random.random((I[k],R)) for k in range(3)]\n",
    "        #print(\"Shape of A: \", A[0].shape, A[1].shape, A[2].shape)\n",
    "        # Unfold X along every mode\n",
    "        Xn = [self.unfold(X,k+1) for k in range(3)]\n",
    "        # Alternative way to initialize A is to take first R left singular vectors from unfolded tensors\n",
    "        #for k in range(3):\n",
    "            #U,S,V = np.linalg.svd(Xn[k])\n",
    "            #A[k] = U[:,0:R]\n",
    "        # Iterative step of ALS\n",
    "        for reps in range(10000):\n",
    "            tmp = A[:]\n",
    "            for k in range(3):# iterate for each mode\n",
    "                idx = [0,1,2]\n",
    "                idx.remove(k)\n",
    "                #print(\"Shape 0 and 1: \",A[idx[0]].shape, A[idx[1]].shape )\n",
    "                Z = self.khatri_rao([A[idx[1]],A[idx[0]]])\n",
    "                W = self.hadamard(mat_mul([A[idx[0]].T,A[idx[0]]]),mat_mul([A[idx[1]].T,A[idx[1]]]))\n",
    "                Winv = np.linalg.pinv(W,rcond=0.001)\n",
    "                A[k] = mat_mul([Xn[k],Z,Winv])         \n",
    "            if (reps+1)%100==0:\n",
    "                err_ = sum([np.linalg.norm(A[p]-tmp[p]) for p in range(3)])\n",
    "                if err_<0.001:\n",
    "                    print(\"The algorithm has converged. Number of iterations in ALS: \",reps)\n",
    "                    break\n",
    "                else:\n",
    "                    tmp = A[:]  \n",
    "        return A # Return A = [[a1,a2,..,aR], [b1,b2,...,bR],[c1,c2,...,cR]]\n",
    "    \n",
    "# Tucker product--------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Tucker product of a tensor with a list of factor matrices\n",
    "    def tucker_product(self,X, M, I): # M = [Mp,Mq,Mr,..,Mn] , I: Index for the corresponding mode I = [p,q,..n]\n",
    "        if len(X.shape)==3:\n",
    "            for i in range(len(I)):\n",
    "                X = self.mode_n_product(X,M[i],I[i])\n",
    "        elif len(X.shape)==4:\n",
    "            for i in range(len(I)):\n",
    "                X = self.mode_n_product4(X,M[i],I[i])\n",
    "            \n",
    "        return X\n",
    "\n",
    "# Tucker Matrix Product ------------------------------------------------------------------------------------------\n",
    "    \n",
    "    def tucker_matrix_product(self,X, M_,n): # M = [M1,M2,M3,..,MN] \n",
    "        M = M_[:]\n",
    "        Mn = M[n-1]\n",
    "        M.pop(n-1)\n",
    "        M.reverse()\n",
    "        M = [M[k].T for k in range(len(M))]\n",
    "        Z = self.kron(M)\n",
    "        Xn = self.unfold(X,n)\n",
    "        Yn = mat_mul([Mn,Xn, Z])\n",
    "        return Yn # Returns the unfolded tensor along mode n after taking tucker product    \n",
    "      \n",
    "# Tucker Decompostion---------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Technique: HOOI(Higher-order orthogonal iteration)    \n",
    "    def tucker3(self,X,R):# Find the decompostion X = G x {A1,A2,A3}\n",
    "            assert len(R)==3, \"Input list should have exactly three elements\"\n",
    "            I = self.shape(X)\n",
    "            A = [np.random.random((I[k],R[k])) for k in range(3)] #Initialize the factors \n",
    "            Xn = [self.unfold(X,n+1) for n in range(3)]\n",
    "            # Iterate till convergence or max_step\n",
    "            max_step = 1000\n",
    "            for reps in range(max_step):\n",
    "                tmp = A[:]\n",
    "                for k in range(3): # for each mode\n",
    "                    idx = [0,1,2]\n",
    "                    idx.remove(k)\n",
    "                    Z = self.kron([A[idx[1]], A[idx[0]]])\n",
    "                    Yk = mat_mul([Xn[k],Z])\n",
    "                    U,S,V = np.linalg.svd(Yk)\n",
    "                    A[k] = (U[:,0:R[k]])# Take the R[k] leading left singular vectors of Yk\n",
    "                if (reps+1)%10==0: # Check for convergence\n",
    "                    err_ = sum([np.linalg.norm(A[p].flatten()-tmp[p].flatten()) for p in range(3)])\n",
    "                    if err_<= 0.001:\n",
    "                        print(\"The algorithm has converged. Number of iterations: \",reps)\n",
    "                        break\n",
    "                    else:\n",
    "                        tmp = A[:]   \n",
    "            At = [A[k].T for k in range(len(A))]\n",
    "            G = self.tucker_product(X,At,[1,2,3])\n",
    "            return (A,G)\n",
    "\n",
    "# Partial Tucker--------------------------------------------------------------------------------------------------------#\n",
    "   \n",
    "    # Find the decompostion X = G x {A1,A2,A3} given one of the factors\n",
    "    def partial_tucker3(self,X,R,A_,mode):\n",
    "            assert len(R)==3, \"Input list should have exactly three elements\"\n",
    "            I = self.shape(X)\n",
    "            A = [np.random.random((I[k],R[k])) for k in range(3)] #Initialize the factors \n",
    "            A[mode-1] = A_\n",
    "            Xn = [self.unfold(X,n+1) for n in range(3)]\n",
    "            # Iterate till convergence or max_step\n",
    "            max_step = 1000\n",
    "            id_ = [0,1,2]\n",
    "            id_.remove(mode-1)\n",
    "            for reps in range(max_step):\n",
    "                tmp = A[:]\n",
    "                for k in id_: # for each mode\n",
    "                    idx = [0,1,2]\n",
    "                    idx.remove(k)\n",
    "                    Z = self.kron([A[idx[1]], A[idx[0]]])\n",
    "                    Yk = mat_mul([Xn[k],Z])\n",
    "                    U,S,V = np.linalg.svd(Yk)\n",
    "                    A[k] = (U[:,0:R[k]])# Take the R[k] leading left singular vectors of Yk\n",
    "\n",
    "                if (reps+1)%10==0: # Check for convergence\n",
    "                    err_ = sum([np.linalg.norm(A[p].flatten()-tmp[p].flatten()) for p in range(3)])\n",
    "                    if err_<= 0.001:\n",
    "                        #print(\"The algorithm has converged. Number of iterations: \",reps)\n",
    "                        break\n",
    "                    else:\n",
    "                        tmp = A[:]   \n",
    "                        \n",
    "            At = [A[k].T for k in range(len(A))]\n",
    "            G = self.tucker_product(X,At,[1,2,3])\n",
    "            #A = [At[k].T for k in range(3)]\n",
    "            return (A,G)\n",
    "            \n",
    "# Covariate Tensor-Tensor Regression------------------------------------------------------------------------------\n",
    "   \n",
    "    # Assumption: The samples are stacked along the third mode of the tensor \n",
    "    def cov_regression(self,X_Data,Y_Data, alpha, R_X, R_Y):\n",
    "        # alpha is between 0 and 1. alpha is approximately (input fitting)/(output correlation) coefficient\n",
    "        # R_X and R_Y are dimensions of tucker factors for X_Data and Y_Data respectively\n",
    "        I_X = self.shape(X_Data)\n",
    "        I_Y = self.shape(Y_Data)\n",
    "        assert I_X[2]==I_Y[2] and R_X[2]==R_Y[2], \"The number of samples must equal the dimension in the third mode\"\n",
    "        C = np.random.random((I_X[2],R_X[2])) # C_X = C_Y\n",
    "        C_old = np.copy(C)\n",
    "        for reps in range(1000):\n",
    "            (Ax,Gx_)= self.partial_tucker3(X_Data,R_X, C, mode=3)\n",
    "            (Ay,Gy_)= self.partial_tucker3(Y_Data,R_Y, C, mode=3)\n",
    "            Gx = self.unfold(Gx_,3)\n",
    "            Gy = self.unfold(Gy_,3)\n",
    "            X_flat = self.unfold(X_Data,3)\n",
    "            Y_flat = self.unfold(Y_Data,3)\n",
    "            Px = mat_mul([Gx,self.kron([Ax[1].T,Ax[0].T])])\n",
    "            Py = mat_mul([Gy,self.kron([Ay[1].T,Ay[0].T])])\n",
    "            X_pinv = np.linalg.pinv(X_flat,rcond=0.001)\n",
    "            Z = np.sqrt(alpha)*mat_mul([X_flat,Px.T])+np.sqrt(1-alpha)*mat_mul([Y_flat,Py.T])\n",
    "            #Alternatively, U = np.sqrt(alpha)*mat_mul([Px,Px.T])+np.sqrt(1-alpha)*mat_mul([Py,Py.T])\n",
    "            U = np.sqrt(alpha)*mat_mul([Gx,Gx.T])+np.sqrt(1-alpha)*mat_mul([Gy,Gy.T])\n",
    "            X_pinv = np.linalg.pinv(X_flat)\n",
    "            U_pinv = np.linalg.pinv(U)\n",
    "            W = mat_mul([X_pinv,Z,U_pinv])\n",
    "            C = mat_mul([X_flat,W])\n",
    "            if (reps+1)%500==0:\n",
    "                if np.linalg.norm(C-C_old)<0.1:\n",
    "                    print(\"The algorithm has converged. Number of iterations: \",reps)\n",
    "                    break\n",
    "        X_fit = mat_mul([C,Px])\n",
    "        Error = np.linalg.norm(X_fit-X_flat)\n",
    "        print(\"Error in input data fit: \", Error, \"and norm of the input data: \",np.linalg.norm(X_flat) )     \n",
    "        return (Ax,Gx,Ay,Gy,W,Py)\n",
    "\n",
    "# Compute Matrix root--------------------------------------------------------------------------------------\n",
    "    # Compute A's from Cov: Cov = U*S*Vh, A = U*Sqrt(S)\n",
    "    def mat_root(self,Cov):\n",
    "        U,s,Vh = np.linalg.svd(Cov)\n",
    "        s_r = np.sqrt(s)\n",
    "        if s_r.any()==0:\n",
    "            print(\"Error: Cov is not conditioned well\")\n",
    "#         s_inv_r = 1/s_r\n",
    "#         s_inv_r[s_inv_r>1000000] = 0\n",
    "#         s_inv_r[s_inv_r<1000000] = 0\n",
    "        S_r = np.diag(s_r)\n",
    "        S_inv_r = np.linalg.inv(S_r)#np.diag(s_inv_r) \n",
    "        A = mat_mul([U,S_r])\n",
    "        A_inv = mat_mul([S_inv_r, U.T])\n",
    "        return (A, A_inv)\n",
    "    \n",
    "# Array-Normal Distribution--------------------------------------------------------------------------------------\n",
    "  \n",
    "    # Separable Covariance estimation for 3-way array data using MLE\n",
    "    def anormal(self, X, coef = 0.5, constraint = False): # X = {X1, X2,...,Xn}\n",
    "        I = self.shape(X[0])\n",
    "        N = len(X)\n",
    "        # Compute the mean:\n",
    "        M = 0\n",
    "        for X_ in X:\n",
    "            M = M + X_\n",
    "        M = M/N\n",
    "        shape_ = list(X[0].shape)\n",
    "        shape_.insert(0,N)\n",
    "        M_ext = np.empty(shape_)\n",
    "        X_ext = np.empty(shape_)\n",
    "        # Extended array mean and array\n",
    "        for i in range(N):\n",
    "            M_ext[i,:,:,:] = M \n",
    "            X_ext[i,:,:,:] = X[i]\n",
    "        # Residual:\n",
    "        E = X_ext - M_ext\n",
    "        # X_i ~ M + Z x {Cov1, Cov2, Cov3}, and CovK = Ak*Ak'\n",
    "        # Intialize the covariance matrices (mode-1, mode-2,mode-3)\n",
    "        # mode-1 covariance = column cov, mode-2 cov= row covariance, mode-3 is pipe cov\n",
    "        Cov = [np.random.rand(I[i],I[i]) for i in range(3)]\n",
    "        Cov[0] = mat_mul([Cov[0],Cov[0].T])\n",
    "       # print(\"Cov[0]: \", Cov[0])\n",
    "        Cov[1] = mat_mul([Cov[1],Cov[1].T])\n",
    "        Cov[2] = mat_mul([Cov[2],Cov[2].T])\n",
    "        if constraint==True:\n",
    "            # Give auto-regressive structure to the longituduinal covariance matrix\n",
    "            corr_ = [coef**j for j in range(I[2])]\n",
    "            Cov[2][0,:] = np.array(corr_)\n",
    "            for j in range(I[2]-1):\n",
    "                el = corr_.pop()\n",
    "                corr_.insert(0,coef**(j+1))\n",
    "                Cov[2][j+1,:] = np.array(corr_)\n",
    "        A = [None]*3\n",
    "        A_inv = [None]*3\n",
    "        for j in range(3):\n",
    "            (A[j],A_inv[j]) = self.mat_root(Cov[j])\n",
    "        A_inv_ext = A_inv[:]\n",
    "        A_inv_ext.append(np.identity(N))\n",
    "        \n",
    "        for reps in range(1000):\n",
    "            tmp = Cov[:]\n",
    "            #pdb.set_trace()\n",
    "            for k in range(2): #iterate over each mode\n",
    "                idx = [0,1]\n",
    "                idx.pop(k)\n",
    "                for j in idx:\n",
    "                    (A[j], A_inv_ext[j]) = self.mat_root(Cov[j])\n",
    "                A_inv_ext[k] = np.identity(I[k])\n",
    "                E_ = self.tucker_product(E,A_inv_ext,[1,2,3,4])\n",
    "                E_k = self.unfold4(E_,k+1)\n",
    "                S = mat_mul([E_k,E_k.T])\n",
    "                nk = N*np.prod(I)/I[k]\n",
    "                Cov[k] = S/nk\n",
    "                \n",
    "            if (reps+1)%10==0:\n",
    "                err_ = sum([np.linalg.norm(Cov[p]-tmp[p]) for p in range(3)])\n",
    "                if err_ < 0.001:\n",
    "                    print(\"MLE converged in \", reps, \" steps\")\n",
    "                    break\n",
    "                else:\n",
    "                    tmp = Cov[:]\n",
    "        return (M,Cov,A)\n",
    "\n",
    "#  Array-normal Conditioning--------------------------------------------------------------------------------------\n",
    "   \n",
    "    # 3-way array-normal conditioning\n",
    "    def anormal_condition(self,M,Cov,Ia,X_a, slice_):\n",
    "        # mode: along which mode unfolding each column is partially known\n",
    "        # Ia: Index of the know data along the given mode\n",
    "        # X_a: data slice at index Ia\n",
    "        # mode = 1,2,3 => incomplete info along each row, columns, pipe respectively\n",
    "        Ix = M.shape\n",
    "        if slice_ == 1:# incomplete info along each row when unfoded along mode 2 OR slice given along mode 1\n",
    "            mode = 2\n",
    "            Ib = set(range(Ix[2]))\n",
    "            Ib = list(Ib - set(Ia)) #index of unknown row elements\n",
    "            Cov_mode = Cov[mode-1] # row covariance i.e. covariance along mode 2 unfolding\n",
    "            M_b = M[:,:,Ib] \n",
    "            M_a = M[:,:,Ia]\n",
    "        elif slice_ == 2:# incomplete info along each column when unfolded along mode 1 OR slice given along mode 2\n",
    "            mode =  1\n",
    "            Ib = set(range(Ix[1]))# index of unknown columns\n",
    "            Ib = list(Ib - set(Ia))\n",
    "            Cov_mode = Cov[mode-1] # row covariance  \n",
    "            M_b = M[:,Ib,:]\n",
    "            M_a = M[:,Ia,:]\n",
    "        elif slice_==3:# incomplete info along each pipe\n",
    "            mode = 3\n",
    "            Ib = set(range(Ix[0]))# index of unknown columns\n",
    "            Ib = list(Ib - set(Ia))\n",
    "            Cov_mode = Cov[mode-1] # row covariance  \n",
    "            M_b = M[Ib,:, :]\n",
    "            M_a = M[Ia,:,:]\n",
    "        else:\n",
    "            raise ValueError('Invalid mode passed')\n",
    "        Cov_ba = Cov_mode[np.ix_(Ib,Ia)]\n",
    "        Cov_aa = Cov_mode[np.ix_(Ia,Ia)]\n",
    "        Cov_bb = Cov_mode[np.ix_(Ib,Ib)]\n",
    "        inv_Cov_aa = np.linalg.inv(Cov_aa)\n",
    "        update_coef = mat_mul([Cov_ba,inv_Cov_aa]) \n",
    "        Cov_ = Cov[:] \n",
    "        Cov_[mode-1] = Cov_bb - mat_mul([update_coef,Cov_ba.T])\n",
    "        M_ = M_b + self.mode_n_product((X_a-M_a),update_coef,mode)  \n",
    "        return (M_,Cov)\n",
    "        \n",
    "# Sampling from array-normal distribution---------------------------------------------------------------------------\n",
    "    \n",
    "    def anormal_sampling(self,M,Cov):\n",
    "        A = [None]*3 #Cov[:]\n",
    "        # Compute the matrix-square-root (Cov = A*A') of covariance matrices\n",
    "        for j in range(3):             \n",
    "            A[j],_ = self.mat_root(Cov[j])\n",
    "        Z = np.random.randn(*M.shape)\n",
    "        for j in range(3):\n",
    "            X_ = M + self.tucker_product(Z,A,[1,2,3])\n",
    "        return X_\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the tensor\n",
    "from tensor_tools import array3\n",
    "ar3 = array3()\n",
    "tensor_shape = [8,3,2] # [mode_1, mode_2 mode_3]: [rows, columns, tubes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The algorithm has converged. Number of iterations in ALS:  299\n",
      "Norm of the Residual:  1.3315481783357157\n",
      "Norm of the Data:  11.60200682748066\n",
      "Number of Parameters in the model:  65\n",
      "Number of elements in the data:  48\n"
     ]
    }
   ],
   "source": [
    "# PARAFAC/CP\n",
    "\n",
    "# Data\n",
    "X = ar3.random_array3(tensor_shape)\n",
    "\n",
    "R = 5 #Number of rank-1 components in the CP\n",
    "\n",
    "#(A,G) = ar3.cp(X,R) #CP decompostion A = [[a1,a2,..,aR], [b1,b2,...,bR],[c1,c2,...,cR]]\n",
    "A = ar3.cp(X,R)\n",
    "\n",
    "# D_ = [np.diag(G[i].flat) for i in range(3)]\n",
    "# D = mat_mul([D_[0],D_[1],D_[2]])#np.identity(R)#\n",
    "\n",
    "X_apprx1 = mat_mul([A[0],(ar3.khatri_rao([A[2],A[1]])).T])\n",
    "\n",
    "Error1 = ar3.unfold(X,1) - X_apprx1\n",
    "\n",
    "print(\"Norm of the Residual: \", np.linalg.norm(Error1))\n",
    "\n",
    "print(\"Norm of the Data: \", np.linalg.norm(X))\n",
    "p = 0\n",
    "q = X.shape[0]*X.shape[1]*X.shape[2]\n",
    "for A_ in A:\n",
    "    p = p + A_.shape[0]*A_.shape[1]\n",
    "\n",
    "print(\"Number of Parameters in the model: \", p)\n",
    "print(\"Number of elements in the data: \", q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The algorithm has converged. Number of iterations:  9\n",
      "Norm of the residual:  1.0266013090758512\n",
      "Norm of the tensor X:  19.347443669826557\n",
      "Number of elements in X:  48\n",
      "Number of Parameters:  55\n"
     ]
    }
   ],
   "source": [
    "# Tucker3 Decomposition \n",
    "\n",
    "# Data\n",
    "X = ar3.random_array3(tensor_shape)\n",
    "rank_ = [3,3,3]\n",
    "\n",
    "# Fit Tucker model\n",
    "(A,G) = ar3.tucker3(X,rank_)\n",
    "X_ = ar3.tucker_product(G,A,[1,2,3])\n",
    "\n",
    "# Residual Analysis\n",
    "print(\"Norm of the residual: \", np.linalg.norm(X-X_))\n",
    "print(\"Norm of the tensor X: \",np.linalg.norm(X))\n",
    "\n",
    "p = 0\n",
    "for A_ in A:\n",
    "    p = p + A_.shape[0]*A_.shape[1]\n",
    "p = p + G.shape[0]*G.shape[1]*G.shape[2]\n",
    "print(\"Number of elements in X: \",X.shape[0]*X.shape[1]*X.shape[2])\n",
    "print(\"Number of Parameters: \", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE converged in  29  steps\n",
      "(2, 8, 3)\n",
      "[(8, 8), (3, 3), (2, 2)]\n"
     ]
    }
   ],
   "source": [
    "# Array-Normal Distribution Test\n",
    "\n",
    "# Generate Data\n",
    "X = [ar3.random_array3(tensor_shape) for k in range(10)]\n",
    "# Or use: X = [np.random.randn(*tensor_shape) for k in range(10)]\n",
    "\n",
    "# Fit array-normal model\n",
    "(M,Cov,A) = ar3.anormal(X)\n",
    "\n",
    "# Sampling from the array-normal distribution \n",
    "Xs = ar3.anormal_sampling(M,Cov)\n",
    "print(M.shape)\n",
    "print([Cov[j].shape for j in range(3)])\n",
    "# Array-normal Conditioning\n",
    "Xt = X[0]\n",
    "slice_ = 3\n",
    "Ia = [0,1]\n",
    "X_a = Xt[Ia,:,:]\n",
    "#M_,Cov_ = ar3.anormal_condition(M,Cov,Ia,X_a, slice_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For alpha {0.3} :\n",
      "Error in input data fit:  13.846750759099939 and norm of the input data:  63.38296447349815\n",
      "Expected error in regression:  11.095367620031979\n",
      "Expected norm of Y:  43.537658230899915\n",
      "For alpha {0.6} :\n",
      "Error in input data fit:  9.273264944292471 and norm of the input data:  63.38296447349815\n",
      "Expected error in regression:  10.883239821742336\n",
      "Expected norm of Y:  43.584117290660075\n",
      "For alpha {0.8999999999999999} :\n",
      "Error in input data fit:  12.643707193427314 and norm of the input data:  63.38296447349815\n",
      "Expected error in regression:  11.859158140142206\n",
      "Expected norm of Y:  43.48871826754525\n"
     ]
    }
   ],
   "source": [
    "# Tensor-Tensor Regression Test\n",
    "\n",
    "# Generate Data\n",
    "X = ar3.random_array3(tensor_shape)\n",
    "Y = X*2 + 0.1\n",
    "\n",
    "# Fit a regression model and test the fit for different alpha (hyperparameter)\n",
    "reps = 0\n",
    "alpha = 0\n",
    "while reps<3:\n",
    "    reps = reps+1\n",
    "    alpha=alpha+0.3\n",
    "    print(\"For alpha %s :\"%{alpha})\n",
    "    R_X=[3,3,5]\n",
    "    R_Y = R_X\n",
    "    (Ax,Gx,Ay,Gy,W,Py)=ar3.cov_regression(X,Y, alpha, R_X, R_Y)\n",
    "    y_mag = 0\n",
    "    err = 0\n",
    "    for k in range(tensor_shape[2]):\n",
    "        X_test = X[k,:,:]\n",
    "        a = mat_mul([W.T, X_test.flatten()])\n",
    "        Y_test = mat_mul([a,Py])\n",
    "        y_mag = y_mag + np.linalg.norm(Y_test)\n",
    "        err = err + np.linalg.norm(Y_test-Y[k,:,:].flatten())\n",
    "\n",
    "    print(\"Expected error in regression: \", err/tensor_shape[2])   \n",
    "    print(\"Expected norm of Y: \", y_mag/tensor_shape[2])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the following code you need tensorly package from http://tensorly.org/ \n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import tucker, parafac, non_negative_tucker\n",
    "rank_ = [6,11,4]\n",
    "(G_,A_) = tucker(X, ranks=rank_)\n",
    "X_ = tl.tucker_to_tensor(G_,A_)\n",
    "print(np.linalg.norm(X_-X))\n",
    "\n",
    "A_ = parafac(X, rank=R)\n",
    "Xa = mat_mul([A_[0],(ar3.khatri_rao([A_[1],A_[2]])).T])\n",
    "X_ = tl.unfold(X,0)\n",
    "print(np.linalg.norm(X_-Xa))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
